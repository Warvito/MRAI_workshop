{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Heart disease is the leading cause of death globally. __Heart failure__ (HF) is the primary cause of heart diseases, and that refers to a physiological state in which the heart is unable to pump sufficiently to maintain blood flow to meet the body's need. It usually occurs because the heart has become too weak or stiff. \n",
    "\n",
    "HF is routinely diagnosed by:\n",
    "* __Electrocardiogram (ECG)__, which measure heart's rhythm and electrical activity. From the ECG the QRS duration can be computed. An increased QRS duration, indicate dyssynchronous contraction and relaxation of the left and right ventricles and it is a marker of detection of HF subjects.\n",
    "* __MRI__ or __US imaging__, which provides structural and functional information of the heart. Evaluation of the structure and the function of the ventricles can provide useful information for diagnosis and characterization of disease. \n",
    " \t \t\n",
    "Based on the result of these tests, doctors use NYHA class to classify patients' heart failure according to the severity of their symptoms:\n",
    "\n",
    "__Class I__: no limitation is experienced in any activities; there are no symptoms from ordinary activities.\n",
    "\n",
    "__Class II__: slight, mild limitation of activity; the patient is comfortable at rest or with mild exertion.\n",
    "\n",
    "__Class III__: marked limitation of any activity; the patient is comfortable only at rest.\n",
    "\n",
    "__Class IV__: any physical activity brings on discomfort and symptoms occur at rest.\n",
    "\n",
    "### Current diagnostic technique\n",
    "\n",
    "Currently, the parameters used in clinics to identify cardiac patients are:\n",
    "* __LVEDV__ which is the maximum amount of blood that heart can pump\n",
    "* __LVESV__ which is the minium amount of blood that heart can pump\n",
    "* __LVSV__ which is the amount of blood pumped by the left ventricle of the heart in one contraction. \n",
    "* __Ejection fraction (LVEF)__, which computes the amount of blood of the left ventricle (LV) pumps out with each contraction. A normal heartâ€™s ejection fraction may be between 50 and 70 percent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this workshop is twofold:\n",
    "\n",
    "1. Improve the quality of a pre-trained segmentation neural network to segment the left ventricle (LV) of the heart\n",
    "2. Train a simple deep neural network (DNN) to classify between healthy and heart failure subjects using clinical metrics, i.e. LVEDV, LVESV, LVSV and LVEF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image segmentaion\n",
    "\n",
    "\n",
    "In this exercise we will segment the left ventricle of the heart in relatively small images using neural networks. Below is the code for setting up a segmentation network and training it. The network isn't very good, so the exercise is to improve the quality of the segmentation by improving the network. \n",
    "\n",
    "The data being used here is derived from the Sunnybrook Dataset (https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/) of cardiac images, filtered to contain only left ventricle myocardium segmentations and reduced in XY dimensions.\n",
    "\n",
    "Tensorflow 2.0 documentation can be found here: https://www.tensorflow.org/beta/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow-gpu==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f scd_lvsegs.npz ] ||  wget https://github.com/Warvito/MRAI_workshop_tensorflow/raw/master/scd_lvsegs.npz\n",
    "\n",
    "data = np.load('scd_lvsegs.npz')  # load all the data from the archive\n",
    "\n",
    "images = data['images']  # images in BHW array order\n",
    "segs = data['segs']  # segmentations in BHW array order\n",
    "\n",
    "images = images.astype(np.float32) / images.max()  # normalize images\n",
    "segs = segs.astype(np.float32)\n",
    "\n",
    "print('Images shape: ' + str(images.shape))\n",
    "print('Segmentation masks shape: ' + str(segs.shape))\n",
    "\n",
    "plt.imshow(images[13] + segs[13] * 0.25, cmap='gray')  # show image 13 with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.reshape(images.shape[0], 64, 64, 1)\n",
    "segs = segs.reshape(segs.shape[0], 64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test\n",
    "\n",
    "For this exercise, we will divide the data between train and test data, choosing to retain the last 6 cases as our test set. Typically you would want to have a training dataset, a validation dataset checked periodically during training to ensure the network continues to produce good results for data is isn't trained on, and a test dataset used to validate the training after the fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 6  # keep the last 6 cases for testing\n",
    "\n",
    "# Divide the images, segmentations, and categories into train/test sets\n",
    "train_images, train_segs = images[:-test_index], segs[:-test_index]\n",
    "test_images, test_segs = images[-test_index:], segs[-test_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "\n",
    "As loss function, for this exercise we will use the Dice loss, which measure the overlap between the ground truth segmentation and the predicted segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred, smooth=1e-5):\n",
    "    \"\"\" Function to calculate the dice loss.\n",
    "\n",
    "    Dice = (2*|X & Y|) / (|X| + |Y|)\n",
    "    \"\"\"   \n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    y_pred = tf.sigmoid(y_pred)\n",
    "\n",
    "    y_true = tf.reshape(y_true, [batch_size, -1])\n",
    "    y_pred = tf.reshape(y_pred, [batch_size, -1])\n",
    "\n",
    "    intersection = tf.multiply(y_pred, y_true)\n",
    "    sums = y_pred + y_true\n",
    "\n",
    "    score = 2.0 * (tf.reduce_sum(intersection, axis=1) + smooth) / (tf.reduce_sum(sums, axis=1) + smooth)  \n",
    "    \n",
    "    return 1 - tf.cast(tf.reduce_sum(score), tf.float32) / tf.cast(batch_size, tf.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segmentation network\n",
    "\n",
    "As neural network, we used a simple autoencoder, which has a downsampling path with 2 layers using maxpooling to reduce dimensions. After the bottom layer the decoding path upsamples using upscaling layers.\n",
    "\n",
    "However, the current network architecture is doing poorly. Modify the current network to improve the segmentation accuracy. Some of the possible solutions are:\n",
    "\n",
    "- Add more filters\n",
    "- Add layers\n",
    "- Add dropout layers\n",
    "- Change activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation network\n",
    "inputs = keras.Input(shape=(64, 64, 1))\n",
    "\n",
    "x = keras.layers.Conv2D(filters=2, kernel_size=3, padding='same', activation='linear', use_bias='False')(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "x = keras.layers.Conv2D(filters=4, kernel_size=3, padding='same', activation='linear', use_bias='False')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "x = keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='linear')(x)\n",
    "\n",
    "x = keras.layers.Conv2D(filters=4, kernel_size=3, padding='same', activation='linear', use_bias='False')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = keras.layers.Conv2D(filters=2, kernel_size=3, padding='same', activation='linear', use_bias='False')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "outputs = keras.layers.Conv2D(filters=1, kernel_size=3, padding='same')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network\n",
    "\n",
    "Once we have defined the network, we need to train it. An strategy is to load the whole dataset into memory at once (setting batch size bigger than training set), but typically this isn't possible due to memory restrictions. A different strategy is to create minibatches for each train step, which means that in each iteration only few image (minibatch) would be used for training.\n",
    "\n",
    "This training will take about ~10 minutes (on colab enviroment). The verbose was set to 0 (mute) to improve the training speed. Try verbose=1 or 2 to check in real time the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "model.compile(loss=dice_coef_loss,\n",
    "              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_epochs = 3000\n",
    "history = model.fit(train_images, train_segs,\n",
    "                    batch_size=512,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (train_images[10])[np.newaxis, :]\n",
    "sample_groud_truth = (train_segs[10])[np.newaxis, :]\n",
    "pred_sample = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample an image from the training data and look at the segmentation the network predicted for it\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].semilogy(history.history['loss'])\n",
    "ax[1].set_title('Sample Image')\n",
    "ax[1].imshow(sample[0, :, :, 0])\n",
    "ax[2].set_title('Sample Ground Truth')\n",
    "ax[2].imshow(sample_groud_truth[0, :, :, 0])\n",
    "ax[3].set_title('Sample Logits')\n",
    "ax[3].imshow(pred_sample[0, :, :, 0])\n",
    "ax[4].set_title('Sample Predicted Segmentation')\n",
    "ax[4].imshow(pred_sample[0, :, :, 0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Once the network is been train, we can now apply the test data to the network. These images were never seen by the network so how well the task is performed is an indicator of how generalized and robust the network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss = model.evaluate(test_images, test_segs,\n",
    "                          batch_size=10)\n",
    "\n",
    "pred = model.predict(test_images)\n",
    "\n",
    "seg = pred > 0.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 5))\n",
    "ax[0].set_title('Test Image')\n",
    "ax[0].imshow(test_images[0, :, :, 0])\n",
    "ax[1].set_title('Ground Truth')\n",
    "ax[1].imshow(test_segs[0, :, :, 0])\n",
    "ax[2].set_title('Logits')\n",
    "ax[2].imshow(pred[0, :, :, 0])\n",
    "ax[3].set_title('Predicted Segmentation')\n",
    "ax[3].imshow(seg[0, :, :, 0])\n",
    "\n",
    "print('Test loss:', testloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification using clinical metrics \n",
    "\n",
    "In this part of the workshop, we aim to use the segmentations produced from the previous segmentation network to compute the following clinical metrics LVEDV, LVESV, LVSV and LVEF to classify the between healthy and heart failure subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN network\n",
    "\n",
    "For classification, we will use a deep neural network classifier (DNN) with two layers with sizes 20 and 10. To evaluate the performances of the classifier we will compute accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, target_names):\n",
    "    \"\"\" Compute metrics.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: labels predicted\n",
    "    y_true: ground truth labels\n",
    "    target_names:  names matching the labels\n",
    "    \"\"\"\n",
    "\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    print('Balanced accuracy  {0:.2f}'.format(bacc))\n",
    "\n",
    "    headers = [\"precision\", \"recall\"]\n",
    "\n",
    "    rows = zip(target_names, precision, recall)\n",
    "\n",
    "    digits = 2\n",
    "    longest_last_line_heading = 'weighted avg'\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' + u' {:>9}\\n'\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "    report += u'\\n'\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    return bacc * 100, precision * 100, recall * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a DNN network for classification with two layers with sizes 20 and 10.\n",
    "Plain dense neural network of linear layers using dropout and ReLU activation.\n",
    "Size of the hidden layers: [20,10]\n",
    "numClasses: number of classes used for the classification'''\n",
    "\n",
    "inputs = keras.Input(shape=(4))\n",
    "x = ...\n",
    "x = ...\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "To ensure that everyone work with the same data, download the following file that contains the images segmented with the previous network optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f data.npz ] || wget https://github.com/Warvito/MRAI_workshop_tensorflow/raw/master/data.npz\n",
    "data_filename = 'data.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute clinical metricas and save them in a matrix called metrics\n",
    "- LVEDV (maximum volume of the cardiac cycle)\n",
    "- LVESV (minimum volume of the cardiac cycle)\n",
    "- LVSV = LVEDV - LVESV \n",
    "- LVEF = LVSV/LVEDV*100 \n",
    "\n",
    "Then:\n",
    "- Generate a matrix ```metrics``` that concatenate LVEDV, LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load(data_filename)\n",
    "\n",
    "# Save fields inside data\n",
    "images = data['images']  # (805, 128, 128)\n",
    "segs = data['segs']  # (805, 128, 128)\n",
    "case_names = data['caseNames']  # 45\n",
    "case_indices = data['caseIndices']  # 45\n",
    "case_type_names = ['Normal', 'Heart Failure']\n",
    "case_types = data['caseTypes']  # 45\n",
    "case_types[case_types == 2] = 0\n",
    "case_voxel_size = data['caseVoxelSize']\n",
    "is_ed_img = data['isEDImg']  # 805\n",
    "seg_types = data['segTypes']  # ['Background', 'LV Pool']\n",
    "\n",
    "# Compute LVEDV, LVESV, LVSV and LVEF\n",
    "metrics = np.zeros((len(case_names), 4))\n",
    "for ii, ind in enumerate(case_indices):\n",
    "    ind_ed_pat = is_ed_img[ind[0]:ind[1]]\n",
    "    img_pat = images[ind[0]:ind[1], :, :]\n",
    "    seg_pat = segs[ind[0]:ind[1], :, :]\n",
    "    img_ed = img_pat[ind_ed_pat, :, :]\n",
    "    img_es = img_pat[~ind_ed_pat, :, :]\n",
    "    seg_ed = seg_pat[ind_ed_pat, :, :]\n",
    "    seg_es = seg_pat[~ind_ed_pat, :, :]\n",
    "    dx, dy, dz = case_voxel_size[ii]\n",
    "    volume_per_voxel = dx * dy * dz * 1e-3\n",
    "    density = 1.05\n",
    "\n",
    "    metrics[ii, 0] = np.sum(np.sum(seg_ed == 1, axis=1)) * volume_per_voxel\n",
    "    metrics[ii, 1] = np.sum(np.sum(seg_es == 1, axis=1)) * volume_per_voxel\n",
    "    metrics[ii, 2] = metrics[ii, 0] - metrics[ii, 1]\n",
    "    metrics[ii, 3] = metrics[ii, 2] / metrics[ii, 0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Plot LVEDV, LVESV, LVSV and LVEF per groups, i.e. LVEDV and LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_class_data(x, y, title, _case_type_names):\n",
    "    plt.scatter(x[y == 0, 0], x[y == 0, 1], c='r', label=_case_type_names[0])\n",
    "    plt.scatter(x[y == 1, 0], x[y == 1, 1], c='b', label=_case_type_names[1])\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "\n",
    "plt.figure(figsize=[14, 5])\n",
    "plt.subplot(121)\n",
    "plot_two_class_data(metrics[:, 0:2], case_types, 'Healthy vs Heart Failure', case_type_names)\n",
    "plt.xlabel('LVEDV')\n",
    "plt.ylabel('LVESV')\n",
    "plt.subplot(122)\n",
    "plot_two_class_data(metrics[:, 2:], case_types, 'Healthy vs Heart Failure', case_type_names)\n",
    "plt.xlabel('LVSV')\n",
    "plt.ylabel('LVEF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test\n",
    "\n",
    "Data is unbalanced (there is no the same number of patients per group). We need to ensure that in the split training/test there is balanced number of classed. To this end use StratifiedShuffleSplit from scikit-learn that will automatically ensure that (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(metrics, case_types):\n",
    "    x_train, x_test = metrics[train_index, :], metrics[test_index, :]\n",
    "    y_train, y_test = case_types[train_index], case_types[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a DNN network to perform classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "# Select the Optimizer with lr = 0.0001\n",
    "optimizer = ...\n",
    "\n",
    "# Select the loss fuction for classification\n",
    "model.compile(loss=...,\n",
    "              optimizer=optimizer)\n",
    "\n",
    "n_epochs = ...\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=...,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy network and compute metrics\n",
    "\n",
    "First deploy the trained netowrk and then compute the following metrics: balanced accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "# Compute the predicted label\n",
    "y_pred = ...\n",
    "# Compute metrics\n",
    "compute_metrics(y_test, y_pred, ['Healthy','Heart Failure'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
