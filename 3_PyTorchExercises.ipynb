{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "PyTorch is a deep learning library built around tensors and tensor operations. A tensor is a multi-dimensional array of values with other parameters, such as gradients, associated with them. Neural networks are composed of tensors and operations which act on them. Loss functions are defined in terms of tensor operations as well. PyTorch provides other classes implementing optimizers and other facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are easily created from Numpy arrays or simple Python lists of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=np.array([[1.,2.],[3.,4.]])\n",
    "print(n)\n",
    "\n",
    "t=torch.tensor(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are evaluated immediately upon an expression being resolved, exactly like Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([1.,2.])\n",
    "y=torch.tensor([3.,4.])\n",
    "\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural layers are composed of multiple tensors. The type `torch.nn.Linear` implements a fully-connected layer from a input size to an output size with a bias value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=torch.nn.Linear(3,4)\n",
    "print(n)\n",
    "\n",
    "print(n.weight.shape,n.weight)\n",
    "print(n.bias.shape,n.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined `n` as mapping values from $R^3$ space to $R^4$ and as we see the tensor defining its weight has the shape `[4,3]` as expected, and the bias has shape `[4]`. \n",
    "\n",
    "Notice that the `requires_grad` value is set to `True` for those tensors, this means their `grad` parameter for storing the gradients during training will be used, that is they represent trainable parameters in a network. Such tensors can be created manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable=torch.tensor([1.,2.],requires_grad=True)\n",
    "\n",
    "print(trainable)\n",
    "print(trainable.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` value is `None` currently since no gradient operation has been done. Using a loss function (in this case $L_1$ or mean absolute error) we can get a tensor which has a gradient to back-propagate onto `trainable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=torch.nn.L1Loss() # create a loss function\n",
    "l=loss(torch.tensor([2.,2.]),trainable) # the difference between `trainable` and this tensor is the \"loss\" here\n",
    "\n",
    "l.backward() # back-propagate the loss\n",
    "\n",
    "print(l.item()) # use .item() to get the the value out of a single-value tensor\n",
    "print(trainable.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gradient value is what an optimizer would use to update the actual values the network contains in an attempt to minimize the loss value `l`. Training a network is the process of generating a prediction, comparing the prediction against the expected output using a loss function, and then having the optimizer update the weights in your network to minimize that difference, then trying that process again. \n",
    "\n",
    "For example, using the `SGD` optimizer get our tensor `trainable` to differ minimally compared to our ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=torch.optim.SGD([trainable],lr=0.25) # lr is the learning rate, ie. how fast we want to change net parameters\n",
    "iters=10\n",
    "target=torch.tensor([2.,2.])\n",
    "\n",
    "for i in range(iters):\n",
    "    opt.zero_grad() # zero gradients\n",
    "    l=loss(target,trainable) # compute loss\n",
    "    l.backward() # back-propagate gradients\n",
    "    opt.step() # apply gradients to values scaled by learning rate\n",
    "    print(l.item())\n",
    "    \n",
    "print(trainable,'is the same as',target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simplified example that could have been done in a much simpler way but it illustrates the steps involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Your Own Modules\n",
    "Modules are the components of networks, containing weights and the code to perform forward and backward passes. We will see the definition of our networks as modules so it's important to understand what PyTorch expects. \n",
    "\n",
    "In general the module definition we will use follows this outline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleName(torch.nn.Module):\n",
    "    def __init__(self): # constructor\n",
    "        super().__init__() # required before defining components\n",
    "        self.net=torch.nn.Linear(4,10)\n",
    "        \n",
    "    def forward(self,x): # defines forward pass of network\n",
    "        x=self.net(x) # do forward pass of components and pass along result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch requires that we define the forward pass of modules, this is done in the `forward()` method which is inherited form `Module`. You can define the backward pass to calculate a derivative for your network however you like but that is beyond the scope here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Example with Iris Dataset\n",
    "The Iris dataset is a list of four measurements of iris flowers drawn from three species. There are 150 instances in total which we will use to train a network to predict the species based on the measurements.\n",
    "\n",
    "More information on the dataset can be found here: https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "First thing to do is download the data if you're in a Colab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f chelsea.png ] || wget https://github.com/ericspod/VPHSummerSchool2019/raw/master/chelsea.png\n",
    "![ -f iris.csv ] || wget https://github.com/ericspod/VPHSummerSchool2019/raw/master/iris.csv\n",
    "![ -f seeds.csv ] || wget https://github.com/ericspod/VPHSummerSchool2019/raw/master/seeds.csv\n",
    "![ -f boston_test.csv ] || wget https://github.com/ericspod/VPHSummerSchool2019/raw/master/boston_test.csv\n",
    "![ -f boston_train.csv ] || wget https://github.com/ericspod/VPHSummerSchool2019/raw/master/boston_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing is to read the dataset into a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# read the data from a file \n",
    "with open('iris.csv') as o: # `with` block opens file and produces file object `o` then closes it on exit\n",
    "    header=o.readline() # read the first line of the file but don't use it for anything\n",
    "    dat=o.read()\n",
    "\n",
    "# replace category names with numbers\n",
    "dat=dat.replace('Iris-setosa','0') \n",
    "dat=dat.replace('Iris-versicolor','1') \n",
    "dat=dat.replace('Iris-virginica','2') \n",
    "\n",
    "dat=np.genfromtxt(io.StringIO(dat),delimiter=',',dtype=np.float32) # convert csv to array\n",
    "print(dat.shape,dat.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array `dat` now contains an array of flower measurements, where the columns are:\n",
    "* Sepal length (cm)\n",
    "* Sepal width (cm) \n",
    "* Petal length (cm)\n",
    "* Petal width (cm) \n",
    "* Species class: Iris Setosa=0, Iris Versicolour=1, Iris Virginica=2\n",
    "\n",
    "The table contains 50 examples of each species in contiguous sections, ie. the first 50 are Iris Setosa. To divide the data into training and test sets we want to take some of each species out to use as test data to verify our model works. If we split each category into 45 for train and 5 for test, we can define the indices for each category to create `train` and `test` tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfEachClass=50 # number of entries per category\n",
    "trainSize=45 # number of entries from each category to keep as training set\n",
    "arrayRange=np.arange(150) # indices over the whole data set\n",
    "\n",
    "# index of each entry within a class (ie. index of first entry of next category starts at 0)\n",
    "perClassIndices=arrayRange%numOfEachClass \n",
    "print(perClassIndices)\n",
    "\n",
    "train=dat[perClassIndices<trainSize] # index by boolean array\n",
    "test=dat[perClassIndices>=trainSize]\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert these arrays into PyTorch tensor types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainT=torch.tensor(train)\n",
    "testT=torch.tensor(test)\n",
    "print(trainT.shape, trainT.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainT` and `testT` tensors have 5 rows, the first 4 are the measurements and the last is the class. When training we want to give the network the measurements and input then compare the results against the known classes. In `trainT` we can reference the measurements only as `trainT[:,:4]` meaning \"everying up to but not including the 4th column\", and the classes `trainT[:,4]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our simple network which goes from 4 input neurons to 5 hidden ones, then from these 5 to 3 output neurons giving the probability the input was a member of each of the 3 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1=torch.nn.Linear(4,5)\n",
    "        self.ln2=torch.nn.Linear(5,3)\n",
    "        self.act=torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.ln1(x)\n",
    "        x=self.ln2(x)\n",
    "        x=self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train our network for a number of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net() # create our instance of the network\n",
    "\n",
    "loss=torch.nn.CrossEntropyLoss() # entropy loss between probability dimensions and categories\n",
    "opt=torch.optim.Adam(net.parameters(),lr=0.001) # an adaptive optimizer that almost always is best\n",
    "\n",
    "iters=200\n",
    "losses=[]\n",
    "\n",
    "for i in range(iters):\n",
    "    opt.zero_grad() # clear out the gradients from the optimizer\n",
    "    cat=net(trainT[:,:4]) # do a forward pass on the network\n",
    "    lossval=loss(cat,trainT[:,4].long()) # calculate loss between prediction and ground truth\n",
    "    # loss function wants values as integers so use .long() to convert\n",
    "    \n",
    "    lossval.backward() # do a backward pass on the network to update weights\n",
    "    opt.step() # apply the updates\n",
    "    \n",
    "    losses.append(lossval.item()) # save the loss value, .item() gets a single value out of a tensor\n",
    "        \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcat=net(testT[:,:4])\n",
    "numCorrect=torch.sum(testcat.max(1)[1]==testT[:,4].long()).item()\n",
    "print('Number of correct predictions:',numCorrect,'out of',testT.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises:\n",
    "1. The parameter `iters` above sets the number of iterations, try changing this to larger or smaller values and see how the results vary.\n",
    "2. The parameter `lr` is the learning rate the optimizer, essentially how fast it tries to move the weights of the network in the right direction. Try larger or smaller values for this as well\n",
    "3. Our network is really simple, containing only 2 layers and an activation. More layers may help get a better result, and using activation between layers (ie. ReLU) may be helpful as well. Try adding activation or more layers, if these help you may want to reduce the number of training iterations to see the difference.\n",
    "4. We've chosen a train:test ratio of 45:5, try having a larger test set and see how this affects performance. \n",
    "5. We haven't randomly shuffled the dataset either, you can experiment with that (search for `numpy.random`) and just choose the last `N` entries and your test set.\n",
    "6. We've also included a `seeds.csv` dataset of seed measurements plus a species category with 3 classes (Kama, Rosa, Canadian). Adapt your code above to use this dataset instead and see how your results vary. Would a different network help? Note that there are a differing number of instances per class which you will have to account for when dividing the training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Exercise\n",
    "\n",
    "The files `boston_train.csv` and `boston_test.csv` contain data for the Boston House Prices Dataset (info here: https://www.kaggle.com/c/boston-housing/overview). This is composed of 13 statistics for various towns followed by the median house price. \n",
    "\n",
    "We want to train a network which predicts the price based on the 13 stats. This is a regression problem where we are attempting to define a continuous function rather than attempting to categorize as the above examples are. What you've seen above and modified during the exercises can be applied to this task as well, what will be different is the activation function your network will use since this isn't a statistical problem, and the loss function since we're not doing a statistical comparison versus a ground truth category assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Classification/Regression\n",
    "\n",
    "In later exercises we will be doing inference on images using convolutional neural networks. It's important to understand what convolutions do and the arguments to the routines we'll use. \n",
    "\n",
    "A 2D convolutional layer is created as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=torch.nn.Conv2d(1,1,3,1,1) # 1 in channel, 1 out channel, kernel size of 3x3, stride of 1, padding 1\n",
    "\n",
    "print(conv.weight.size(), conv.bias.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the `weight` tensor is `[out_channel, in_channel, kernel_width, kernel_height]` so `[1,1,3,3]` is expected given the constructor values. A stride of 1 means the sliding window moves over by 1 pixel, and a padding value of 1 ensures the output for a 3x3 kernel is the same size as the input, otherwise the output would be 2 pixels smaller since kernels cannot go over image edges.\n",
    "\n",
    "The `conv` object acts like a callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.rand((1,1,32,32))\n",
    "\n",
    "print(conv(t).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define a convolution with a different number of output channels, with a different stride, or different padding value this results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2=torch.nn.Conv2d(1,2,3)\n",
    "print(conv2.weight.size(),conv2(t).size())\n",
    "conv3=torch.nn.Conv2d(1,2,5)\n",
    "print(conv3.weight.size(),conv3(t).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the weights of this kernel directly and apply it to a tensor, for example one loaded from an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat=plt.imread('chelsea.png')\n",
    "catred=cat[:,:,0] # take red channel\n",
    "catbatch=catred[None,None] # tensors must have a batch and channel dimension before height and width\n",
    "\n",
    "print(catbatch.shape)\n",
    "\n",
    "cattensor=torch.tensor(catbatch)\n",
    "\n",
    "# set convolution kernel to be 1 in middle and otherwise 0 \n",
    "conv.weight[0,0,:,:]=torch.tensor([[0,0,0],[0,1,0],[0,0,0]])\n",
    "conv.bias[0]=1 # should set the bias as well\n",
    "\n",
    "catout=conv(cattensor)\n",
    "catnumpy=catout.data.numpy() # convert to numpy\n",
    "\n",
    "print(catnumpy.shape)\n",
    "\n",
    "plt.imshow(np.squeeze(catnumpy),cmap='gray') # squeeze removes dimensions with size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernel should produce an image that's identical to the original. Other kernels can be used to do edge detection or gradient detection. \n",
    "\n",
    "* Try setting different values to the kernel, for example using a Sobel operator (https://en.wikipedia.org/wiki/Sobel_operator) or Laplace operator (https://en.wikipedia.org/wiki/Laplace_operator).\n",
    "* Experiment with kernels of different sizes and setting their values to produce different results from our cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv.weight[0,0,:,:]= ...something...\n",
    "\n",
    "catout=conv(cattensor)\n",
    "plt.imshow(np.squeeze(catout.data.numpy()),cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
